{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project': 'CCF_HCD_STG',\n",
       " 'collections': ['Structural_preproc'],\n",
       " 'ignore_list': ['OTHER_FILES'],\n",
       " 'subjects': ['HCD0737657', 'HCD0551239'],\n",
       " 'sessions': []}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "from copy import copy\n",
    "from hashlib import md5\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import yaxil\n",
    "from yaxil.exceptions import RestApiError\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Download from Remote XNAT')\n",
    "parser.add_argument('--project', '-p', type=str)\n",
    "parser.add_argument('--collections', '-c', type=str, nargs='+')\n",
    "parser.add_argument('--ignore-list', type=str, nargs='+', default=['OTHER_FILES'])\n",
    "parser.add_argument('--subjects', '-s', type=str, nargs='+', default=[], help='Explicit list of subjects')\n",
    "parser.add_argument('--sessions', type=list, default=[], help='Explicit list of sessions')\n",
    "\n",
    "os.chdir('/net/holynfs01/srv/export/ncf_hcp/share_root/data/intradb/')\n",
    "         \n",
    "args = parser.parse_args(['-p', 'CCF_HCD_STG',  '-s', 'HCD0737657', 'HCD0551239', '-c', 'Structural_preproc'])\n",
    "opts = vars(args)\n",
    "opts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'project': 'CCF_HCD_STG',\n",
       " 'collections': ['Structural_preproc'],\n",
       " 'ignore_list': ['OTHER_FILES'],\n",
       " 'subjects': ['HCD0737657', 'HCD0551239'],\n",
       " 'sessions': []}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "locals().update(opts)\n",
    "opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Fetching list of experiments\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/subjects\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/subjects?columns=ID%2Clabel%2Cproject&label=HCD0737657&project=CCF_HCD_STG HTTP/1.1\" 200 None\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/subjects\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with subject HCD0737657\n",
      "no records returned from https://intradb.humanconnectome.org/data/subjects?columns=ID%2Clabel%2Cproject&label=HCD0737657&project=CCF_HCD_STG\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/subjects?columns=ID%2Clabel%2Cproject&label=HCD0551239&project=CCF_HCD_STG HTTP/1.1\" 200 None\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments?columns=ID%2Clabel%2Cproject%2Cxnat%3Asubjectassessordata%2Fsubject_id%2Csubject_label%2Cinsert_date&project=CCF_HCD_STG&xnat%3Asubjectassessordata%2Fsubject_id=HCPIntradb04_S00675 HTTP/1.1\" 200 None\n",
      "INFO:__main__:Found 3 experiments\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[OrderedDict([('uri', '/data/experiments/HCPIntradb04_E05943'),\n",
       "              ('label', 'HCD0551239_V1_MR'),\n",
       "              ('id', 'HCPIntradb04_E05943'),\n",
       "              ('project', 'CCF_HCD_STG'),\n",
       "              ('subject_id', 'HCPIntradb04_S00675'),\n",
       "              ('subject_label', 'HCD0551239'),\n",
       "              ('archived_date', '2018-12-04 20:00:32.549')]),\n",
       " OrderedDict([('uri', '/data/experiments/HCPIntradb09_E01002'),\n",
       "              ('label', 'HCD0551239_V2_MR'),\n",
       "              ('id', 'HCPIntradb09_E01002'),\n",
       "              ('project', 'CCF_HCD_STG'),\n",
       "              ('subject_id', 'HCPIntradb04_S00675'),\n",
       "              ('subject_label', 'HCD0551239'),\n",
       "              ('archived_date', '2020-07-02 17:17:10.525')]),\n",
       " OrderedDict([('uri', '/data/experiments/HCPIntradb14_E00691'),\n",
       "              ('label', 'HCD0551239_V3_MR'),\n",
       "              ('id', 'HCPIntradb14_E00691'),\n",
       "              ('project', 'CCF_HCD_STG'),\n",
       "              ('subject_id', 'HCPIntradb04_S00675'),\n",
       "              ('subject_label', 'HCD0551239'),\n",
       "              ('archived_date', '2020-07-02 10:01:56.039')])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if not ignore_list:\n",
    "    ignore_list = list()\n",
    "if not subjects:\n",
    "    subjects = list()\n",
    "if not sessions:\n",
    "    sessions = list()\n",
    "auth = yaxil.auth('intradb')  # Requires setup and description\n",
    "start_time = time.time()\n",
    "with yaxil.session(auth) as sess:\n",
    "    if not sessions:\n",
    "        subject_labels = subjects\n",
    "        logger.info('Fetching list of experiments')\n",
    "        experiments = []\n",
    "        \n",
    "        for label in subject_labels:\n",
    "            try:\n",
    "                sub = list(sess.subjects(label=label, project=project))[0]\n",
    "                experiments.extend(sess.experiments(subject=sub))\n",
    "            except Exception as err:\n",
    "                print('Error with subject {}'.format(label))\n",
    "                print(str(err))\n",
    "        logger.info('Found {} experiments'.format(len(experiments)))\n",
    "        \n",
    "[x._asdict() for x in experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Syncing experiment HCD0015417_V1_MR\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/projects/CCF_HCD_STG/subjects/HCD0015417/experiments/HCD0015417_V1_MR/resources\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/projects/CCF_HCD_STG/subjects/HCD0015417/experiments/HCD0015417_V1_MR/resources?format=json HTTP/1.1\" 200 None\n"
     ]
    }
   ],
   "source": [
    "exp_info = experiments[0]._asdict()\n",
    "\n",
    "with yaxil.session(auth) as sess:\n",
    "    logger.info('Syncing experiment {}'.format(exp_info['label']))\n",
    "    start_time = time.time()\n",
    "    resources_url_pat = ('data/projects/{project}/subjects/{subject_label}/'\n",
    "                         'experiments/{label}/resources')\n",
    "    base_url = resources_url_pat.format(**exp_info)\n",
    "    _, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "\n",
    "    # Filter only wanted collections or return all\n",
    "    if collections:\n",
    "        resources = [\n",
    "            result for result in response['ResultSet']['Result']\n",
    "            if result['label'] in collections\n",
    "        ]\n",
    "    else:\n",
    "        resources = response['ResultSet']['Result']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': '',\n",
       "  'tags': '',\n",
       "  'cat_id': 'HCPIntradb04_E05816',\n",
       "  'element_name': 'xnat:resourceCatalog',\n",
       "  'category': 'resources',\n",
       "  'file_size': '4500978357',\n",
       "  'xnat_abstractresource_id': '2710820',\n",
       "  'file_count': '831',\n",
       "  'label': 'Structural_preproc',\n",
       "  'format': '',\n",
       "  'cat_desc': ' '}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OTHER_FILES']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource_info = resources[0]\n",
    "ignore_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HCD0015417_V1_MR/Structural_preproc/SUCCESS\n",
      "True\n",
      "/net/holynfs01/srv/export/ncf_hcp/share_root/data/intradb\n"
     ]
    }
   ],
   "source": [
    "if not ignore_list:\n",
    "    ignore_list = list()\n",
    "    # Use a cookie to mark a resource as complete\n",
    "success_cookie = os.path.join(exp_info['label'], resource_info['label'],\n",
    "                              'SUCCESS')\n",
    "print(success_cookie)\n",
    "print(os.path.exists(success_cookie))\n",
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_url_pat = (\n",
    "    'data/projects/{project}/subjects/{subject_label}/'\n",
    "    'experiments/{label}/resources/{xnat_abstractresource_id}/files')\n",
    "url_info = copy(exp_info)  # Combine resource and experiment\n",
    "url_info['xnat_abstractresource_id'] = resource_info[\n",
    "    'xnat_abstractresource_id']\n",
    "\n",
    "base_url = resource_url_pat.format(**url_info)\n",
    "_, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "\n",
    "filelist = response['ResultSet']['Result']\n",
    "filelist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Downloading ' + filelist[0]['URI'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_file(uri, ignores_list):\n",
    "    ignore = False\n",
    "    for ignore_pat in ignores_list:\n",
    "        if re.search(ignore_pat, uri):\n",
    "            ignore = True\n",
    "    return ignore\n",
    "def fetch_resource(sess, exp_info, resource_info, always_checksum=False, ignore_list=None):\n",
    "    if not ignore_list:\n",
    "        ignore_list = list()\n",
    "    # Use a cookie to mark a resource as complete\n",
    "    success_cookie = os.path.join(exp_info['label'], resource_info['label'],\n",
    "                                  'SUCCESS')\n",
    "    if os.path.exists(success_cookie) and not always_checksum:\n",
    "        return\n",
    "\n",
    "    resource_url_pat = (\n",
    "        'data/projects/{project}/subjects/{subject_label}/'\n",
    "        'experiments/{label}/resources/{xnat_abstractresource_id}/files')\n",
    "    url_info = copy(exp_info)  # Combine resource and experiment\n",
    "    url_info['xnat_abstractresource_id'] = resource_info[\n",
    "        'xnat_abstractresource_id']\n",
    "\n",
    "    base_url = resource_url_pat.format(**url_info)\n",
    "    _, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "\n",
    "    filelist = response['ResultSet']['Result']\n",
    "    if not len(filelist):\n",
    "        raise ValueError('No files could be read from {} in json response: {}'.format(base_url, response))\n",
    "    logger.info('Syncing {} file (resources) from {}'.format(len(filelist), base_url))\n",
    "\n",
    "    start_time = time.time()\n",
    "    for fileinfo in filelist:\n",
    "        try:\n",
    "            # Rename URI (python variable case)\n",
    "            fileinfo['uri'] = fileinfo.pop('URI')\n",
    "            if ignore_file(fileinfo['uri'], ignore_list):\n",
    "                logger.debug('Ignoring {}'.format(fileinfo['uri']))\n",
    "                continue\n",
    "            print('Downloading ' + fileinfo['uri'])\n",
    "            download_file(sess, out_dir=exp_info['label'], **fileinfo)\n",
    "        except RuntimeError:\n",
    "            logger.info('Digest failed on {}'.format(fileinfo['uri']))\n",
    "        except RestApiError as err:\n",
    "            logger.info('Download Error on {}: {}'.format(\n",
    "                fileinfo['uri'], err))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info('Finished in {}'.format(\n",
    "        time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n",
    "\n",
    "    # if we got here, mark this collection as completed (\"touch\" cookie)\n",
    "    open(success_cookie, 'w').close()\n",
    "def download_file(sess,\n",
    "                  uri,\n",
    "                  digest,\n",
    "                  collection,\n",
    "                  out_dir='.',\n",
    "                  overwrite=False,\n",
    "                  **kwargs):\n",
    "    basename = uri.split('files/')[-1]\n",
    "    fname = os.path.join(out_dir, collection, basename)\n",
    "    dirname = os.path.dirname(fname)\n",
    "\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as f:\n",
    "            disk_digest = md5(f.read()).hexdigest()\n",
    "        if disk_digest == digest:\n",
    "            logger.debug('Digest matched - Skipping {}'.format(fname))\n",
    "            return\n",
    "        elif overwrite:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            logger.info(\n",
    "                'Digest failed - removing {} and trying again'.format(fname))\n",
    "            os.remove(fname)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                '{} exisited with incorrect digest '.format(fname) +\n",
    "                'but cowardly moving on')\n",
    "\n",
    "    try:\n",
    "        _, result = yaxil._get(\n",
    "            sess._auth,\n",
    "            uri,\n",
    "            yaxil.Format.JSON,  # Format is ignored for _file_ downloads\n",
    "            autobox=False)\n",
    "    except RestApiError as err:\n",
    "        # Empty responses are acceptable for some scripts and onset files\n",
    "        if 'response is empty' in str(err):\n",
    "            result = bytes('', 'utf8')\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    with open(fname, 'wb') as f:\n",
    "        f.write(result)\n",
    "\n",
    "    with open(fname, 'rb') as f:\n",
    "        disk_digest = md5(f.read()).hexdigest()\n",
    "    if disk_digest != digest:\n",
    "        retries = kwargs.get('retries', 0)\n",
    "        if overwrite:\n",
    "            os.remove(fname)\n",
    "        if retries >= MAX_RETRIES:\n",
    "            raise RuntimeError(\n",
    "                'Digest failed - ' +\n",
    "                '{} may need to be re-downloaded.'.format(fname))\n",
    "        else:\n",
    "            retries += 1\n",
    "            download_file(sess,\n",
    "                          uri,\n",
    "                          digest,\n",
    "                          collection,\n",
    "                          out_dir,\n",
    "                          overwrite=True,\n",
    "                          retries=retries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(filelist[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ignore_list)\n",
    "print(resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileinfo = filelist[1]\n",
    "fileinfo['uri'] = fileinfo.pop('URI')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_file(sess, out_dir=exp_info['label'], **fileinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fileinfo in filelist:\n",
    "    try:\n",
    "        # Rename URI (python variable case)\n",
    "        fileinfo['uri'] = fileinfo.pop('URI')\n",
    "        if ignore_file(fileinfo['uri'], ignore_list):\n",
    "            logger.debug('Ignoring {}'.format(fileinfo['uri']))\n",
    "            continue\n",
    "        download_file(sess, out_dir=exp_info['label'], **fileinfo)\n",
    "    except RuntimeError:\n",
    "        logger.info('Digest failed on {}'.format(fileinfo['uri']))\n",
    "    except RestApiError as err:\n",
    "        logger.info('Download Error on {}: {}'.format(\n",
    "            fileinfo['uri'], err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fetch_resource(sess, exp_info, resources[0], always_checksum=True, ignore_list=ignore_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for resource in resources:\n",
    "    try:\n",
    "        fetch_resource(sess, exp_info, resource, always_checksum=True, ignore_list=ignore_list)\n",
    "    except ValueError as err:\n",
    "        if 'No JSON object could be decoded' in str(err):\n",
    "            logger.error(err)\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    except ConnectionError as err:\n",
    "        logger.error(err)\n",
    "        resource_errors.append('ConnectionError: {}'.format(\n",
    "            exp_info['label']))\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-hcpl]",
   "language": "python",
   "name": "conda-env-.conda-hcpl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
