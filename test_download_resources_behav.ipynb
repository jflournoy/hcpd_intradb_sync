{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8254eb3a-4935-4476-bd61-45ab42da022b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Download Resources for a project\n",
    "\n",
    "import argparse\n",
    "from copy import copy\n",
    "from hashlib import md5\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import yaxil\n",
    "from yaxil.exceptions import RestApiError\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "\n",
    "def main(project, collections, ignore_list=None, subjects=None, sessions=None, no_checksum=False, psychopy=False, scantype=None, subcollection=None, like_itk=None, file_regex=None):\n",
    "    if not ignore_list:\n",
    "        ignore_list = list()\n",
    "    if not subjects:\n",
    "        subjects = list()\n",
    "    if not sessions:\n",
    "        sessions = list()\n",
    "    if not scantype:\n",
    "        scantype = list()\n",
    "    if not subcollection:\n",
    "        subcollection = list()\n",
    "    if no_checksum:\n",
    "        always_checksum=False\n",
    "    else:\n",
    "        always_checksum=True\n",
    "    logger.info('Project is {}.'.format(project))\n",
    "    if psychopy:\n",
    "       logger.info('Downloading psychopy files from intake project')\n",
    "    auth = yaxil.auth('intradb')  # Requires setup and description\n",
    "    start_time = time.time()\n",
    "    with yaxil.session(auth) as sess:\n",
    "        if not sessions:\n",
    "            experiments = fetch_experiments(sess, project, subjects)\n",
    "\n",
    "        for exp_info in [e._asdict() for e in experiments]:\n",
    "            try:\n",
    "                fetch_experiment(sess, collections, exp_info, ignore_list, always_checksum, psychopy=psychopy, scantype=scantype, subcollection=subcollection, project=project, like_itk=like_itk, file_regex=file_regex)\n",
    "            except Exception as err:\n",
    "                logger.error('(Main) Error with subject {}: {}'.format(exp_info['label'], err))\n",
    "                continue\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info('Finished {} experiments in {}'.format(\n",
    "        len(experiments), time.strftime(\"%H:%M:%S\",\n",
    "                                        time.gmtime(elapsed_time))))\n",
    "\n",
    "\n",
    "def fetch_experiments(sess, project, subject_labels):\n",
    "    \"\"\"Fetch a list of yaxil.Experiment's (or get all for a project).\"\"\"\n",
    "    logger.info('Fetching list of experiments')\n",
    "    if len(subject_labels):\n",
    "        experiments = []\n",
    "        for label in subject_labels:\n",
    "            try:\n",
    "                sub = list(sess.subjects(label=label, project=project))[0]\n",
    "                experiments.extend(sess.experiments(subject=sub))\n",
    "            except Exception as err:\n",
    "                print('(fetch_experiments) Error with subject {}'.format(label))\n",
    "                print(str(err))\n",
    "    else:\n",
    "        experiments = list(sess.experiments(project=project))\n",
    "    logger.info('Found {} experiments'.format(len(experiments)))\n",
    "    return experiments\n",
    "\n",
    "def fetch_experiment(sess, collections, exp_info, ignore_list, always_checksum, psychopy=False, scantype=None, subcollection=None, project=None, like_itk=None, file_regex=None):\n",
    "    logger.info('Syncing experiment {}'.format(exp_info['label']))\n",
    "    start_time = time.time()\n",
    "\n",
    "    # auth = yaxil.XnatAuth(url='...', username='...', password='...')\n",
    "\n",
    "    # url = 'data/experiments/HCPIntradb02_E08367/files'  # ?format=json'\n",
    "    # exp_info = dict(\n",
    "    #     project='CCF_HCD_STG',\n",
    "    #     subject_label='HCD0021614',\n",
    "    #     label='HCD0021614_V1_MR')  # experiment_label\n",
    "\n",
    "    try:\n",
    "        resources = fetch_resources(sess, exp_info, collections, psychopy=psychopy, scantype=scantype, subcollection=subcollection, project=project, like_itk=like_itk)\n",
    "    except ValueError:\n",
    "        logger.error('Unrecoverable error in {}'.format(exp_info['label']))\n",
    "        resources = []\n",
    "\n",
    "    resource_errors = []\n",
    "\n",
    "    for resource in resources:\n",
    "        logger.debug('Fetching all the resources...')\n",
    "        try:\n",
    "            fetch_resource(sess, exp_info, resource, always_checksum=always_checksum, ignore_list=ignore_list, psychopy=psychopy, scantype=scantype, subcollection=subcollection, project=project, like_itk=like_itk, file_regex=file_regex)\n",
    "        except ValueError as err:\n",
    "            if 'No JSON object could be decoded' in str(err):\n",
    "                logger.error(err)\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "        except ConnectionError as err:\n",
    "            logger.error(err)\n",
    "            resource_errors.append('ConnectionError: {}'.format(\n",
    "                exp_info['label']))\n",
    "            continue\n",
    "\n",
    "    with open('errors.log', 'a') as f:\n",
    "        f.writelines(resource_errors)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info('Finished experiment {} in {}'.format(\n",
    "        exp_info['label'], time.strftime(\"%H:%M:%S\",\n",
    "                                         time.gmtime(elapsed_time))))\n",
    "\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    # url_pat = ('data/projects/{project}/subjects/{subject_label}/experiments/'\n",
    "    #            '{label}/files')\n",
    "    #\n",
    "    # base_url = url_pat.format(**exp_info)\n",
    "    # logger.info('Syncing {}'.format(base_url))\n",
    "\n",
    "    # import pdb\n",
    "    # pdb.set_trace()\n",
    "    # # sess.get(base_url, yaxil.Format.JSON)\n",
    "    # _, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "    #\n",
    "    # for collection in collections:\n",
    "    #     import pdb\n",
    "    #     pdb.set_trace()\n",
    "    #     logger.info('Downloading {}'.format(collection))\n",
    "    #     filelist = [\n",
    "    #         r for r in response['ResultSet']['Result']\n",
    "    #         if r['collection'] == collection\n",
    "    #     ]\n",
    "\n",
    "\n",
    "def fetch_resources(sess, exp_info, collections=None, psychopy=False, scantype=None, subcollection=None, project=None, like_itk=None):\n",
    "    \"\"\"Fetch a list of json resources with collection label and id.\"\"\"\n",
    "    if project == 'CCF_HCD_ITK' or like_itk:\n",
    "        resources_url_pat = ('data/projects/{project}/subjects/{subject_label}/'\n",
    "                         'experiments/{label}/scans')\n",
    "    else:\n",
    "        resources_url_pat = ('data/projects/{project}/subjects/{subject_label}/'\n",
    "                             'experiments/{label}/resources')\n",
    "    base_url = resources_url_pat.format(**exp_info)\n",
    "    \n",
    "    _, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "\n",
    "    # Filter only wanted collections or return all\n",
    "    if collections:\n",
    "        if project == 'CCF_HCD_ITK' or like_itk:\n",
    "            if scantype:\n",
    "                resources = [\n",
    "                    result for result in response['ResultSet']['Result']\n",
    "                    if result['series_description'] in collections and result['type'] in scantype\n",
    "                ]\n",
    "            else:\n",
    "                resources = [\n",
    "                    result for result in response['ResultSet']['Result']\n",
    "                    if result['series_description'] in collections\n",
    "                ]\n",
    "        else:\n",
    "            resources = [\n",
    "                result for result in response['ResultSet']['Result']\n",
    "                if result['label'] in collections\n",
    "            ]\n",
    "    else:\n",
    "        resources = response['ResultSet']['Result']\n",
    "\n",
    "    if not len(resources):\n",
    "        msg = 'Found no resources '\n",
    "        if collections:\n",
    "            msg += 'matching collections {} '.format(collections)\n",
    "        msg += 'for {}'.format(base_url)\n",
    "        logger.warning(msg)\n",
    "    else:\n",
    "        logger.info('Found {} resources matching {}.'.format(str(len(resources)), collections))\n",
    "\n",
    "    return resources\n",
    "\n",
    "\n",
    "def fetch_resource(sess, exp_info, resource_info, always_checksum=False, ignore_list=None, psychopy=False, scantype=None, subcollection=None, project=None, like_itk=None, file_regex=None):\n",
    "    \n",
    "    if not ignore_list:\n",
    "        ignore_list = list()\n",
    "    # Use a cookie to mark a resource as complete\n",
    "    if project == 'CCF_HCD_ITK' or like_itk:\n",
    "        resource_info_dir_index = 'series_description'\n",
    "    else:\n",
    "        resource_info_dir_index = 'label'\n",
    "    success_cookie = os.path.join(exp_info['label'], resource_info[resource_info_dir_index],\n",
    "                                  'SUCCESS')\n",
    "    logger.debug('Fetching resource {}'.format(resource_info[resource_info_dir_index]))\n",
    "    if os.path.exists(success_cookie) and not always_checksum:\n",
    "        return\n",
    "    \n",
    "    if psychopy:\n",
    "        logger.debug('Using intake database URI to fetch resource')\n",
    "        resource_url_pat = (resource_info['URI'] + '/resources/LINKED_DATA/files')\n",
    "        base_url = resource_url_pat\n",
    "    elif project == 'CCF_HCD_ITK' or like_itk:\n",
    "        logger.debug('Using intake database URI to fetch resource')\n",
    "        resource_url_pat = (resource_info['URI'] + '/files')\n",
    "        base_url = resource_url_pat\n",
    "    else:\n",
    "        resource_url_pat = (\n",
    "            'data/projects/{project}/subjects/{subject_label}/'\n",
    "            'experiments/{label}/resources/{xnat_abstractresource_id}/files')\n",
    "        url_info = copy(exp_info)  # Combine resource and experiment\n",
    "        url_info['xnat_abstractresource_id'] = resource_info[\n",
    "            'xnat_abstractresource_id']\n",
    "        base_url = resource_url_pat.format(**url_info)\n",
    "        \n",
    "    _, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "\n",
    "    filelist = response['ResultSet']['Result']\n",
    "    if subcollection:\n",
    "        filelist = [\n",
    "            file for file in filelist if file['collection'] in subcollection\n",
    "        ]\n",
    "    if file_regex:\n",
    "        filelist = [\n",
    "            file for file in filelist if re.search(file_regex, file['Name'])\n",
    "        ]\n",
    "        \n",
    "    if not len(filelist):\n",
    "        raise ValueError('No files could be read from {} in json response: {}'.format(base_url, response))\n",
    "    logger.info('Syncing {} file (resources) from {}'.format(len(filelist), base_url))\n",
    "\n",
    "    start_time = time.time()\n",
    "    for fileinfo in filelist:\n",
    "        try:\n",
    "            # Rename URI (python variable case)\n",
    "            fileinfo['uri'] = fileinfo.pop('URI')\n",
    "            if ignore_file(fileinfo['uri'], ignore_list):\n",
    "                logger.debug('Ignoring {}'.format(fileinfo['uri']))\n",
    "                continue\n",
    "            if project == 'CCF_HCD_ITK' or like_itk:\n",
    "                out_dir = os.path.join(exp_info['label'], resource_info[resource_info_dir_index])\n",
    "            else:\n",
    "                out_dir = exp_info['label']\n",
    "            download_file(sess, out_dir=out_dir, **fileinfo)\n",
    "        except RuntimeError:\n",
    "            logger.info('Digest failed on {}'.format(fileinfo['uri']))\n",
    "        except RestApiError as err:\n",
    "            logger.info('Download Error on {}: {}'.format(\n",
    "                fileinfo['uri'], err))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info('Finished in {}'.format(\n",
    "        time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n",
    "\n",
    "    # if we got here, mark this collection as completed (\"touch\" cookie)\n",
    "    open(success_cookie, 'w').close()\n",
    "\n",
    "\n",
    "def download_file(sess,\n",
    "                  uri,\n",
    "                  digest,\n",
    "                  collection,\n",
    "                  out_dir='.',\n",
    "                  overwrite=False,\n",
    "                  **kwargs):\n",
    "    basename = uri.split('files/')[-1]\n",
    "    fname = os.path.join(out_dir, collection, basename)\n",
    "    dirname = os.path.dirname(fname)\n",
    "\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as f:\n",
    "            disk_digest = md5(f.read()).hexdigest()\n",
    "        if disk_digest == digest:\n",
    "            logger.debug('Digest matched - Skipping {}'.format(fname))\n",
    "            return\n",
    "        elif overwrite:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            logger.info(\n",
    "                'Digest failed - removing {} and trying again'.format(fname))\n",
    "            os.remove(fname)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                '{} exisited with incorrect digest '.format(fname) +\n",
    "                'but cowardly moving on')\n",
    "\n",
    "    try:\n",
    "        _, result = yaxil._get(\n",
    "            sess._auth,\n",
    "            uri,\n",
    "            yaxil.Format.JSON,  # Format is ignored for _file_ downloads\n",
    "            autobox=False)\n",
    "    except RestApiError as err:\n",
    "        # Empty responses are acceptable for some scripts and onset files\n",
    "        if 'response is empty' in str(err):\n",
    "            result = bytes('', 'utf8')\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    with open(fname, 'wb') as f:\n",
    "        logger.info('Writing {}'.format(fname))\n",
    "        f.write(result)\n",
    "\n",
    "    with open(fname, 'rb') as f:\n",
    "        disk_digest = md5(f.read()).hexdigest()\n",
    "    if disk_digest != digest:\n",
    "        retries = kwargs.get('retries', 0)\n",
    "        if overwrite:\n",
    "            os.remove(fname)\n",
    "        if retries >= MAX_RETRIES:\n",
    "            raise RuntimeError(\n",
    "                'Digest failed - ' +\n",
    "                '{} may need to be re-downloaded.'.format(fname))\n",
    "        else:\n",
    "            retries += 1\n",
    "            download_file(sess,\n",
    "                          uri,\n",
    "                          digest,\n",
    "                          collection,\n",
    "                          out_dir,\n",
    "                          overwrite=True,\n",
    "                          retries=retries)\n",
    "\n",
    "\n",
    "def ignore_file(uri, ignores_list):\n",
    "    ignore = False\n",
    "    for ignore_pat in ignores_list:\n",
    "        if re.search(ignore_pat, uri):\n",
    "            ignore = True\n",
    "    return ignore\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='Download from Remote XNAT')\n",
    "    parser.add_argument('--project', '-p', type=str)\n",
    "    parser.add_argument('--collections', '-c', type=str, nargs='+')\n",
    "    parser.add_argument('--ignore-list', type=str, nargs='+', default=['OTHER_FILES'])\n",
    "    parser.add_argument('--subjects', '-s', type=str, nargs='+', default=[], help='Explicit list of subjects')\n",
    "    parser.add_argument('--sessions', type=list, default=[], help='Explicit list of sessions')\n",
    "    parser.add_argument('--no-checksum', action='store_true')\n",
    "    parser.add_argument('--scantype', '-t', type=str, nargs='+', help='Specify scan type; useful for intake project.')\n",
    "    parser.add_argument('--subcollection', '-C', type=str, nargs='+', help='Specify sub-collection; useful for intake project.')\n",
    "    parser.add_argument('--psychopy', action='store_true', help='Download behavior data from intake project for specified collections')\n",
    "    parser.add_argument('--like-itk', action='store_true', help='Download using intake-style urls (useful for getting special scan data even from staging project)')\n",
    "    parser.add_argument('--file-regex', '-r', type=str, help='Grab files by regex. This regex is not checked!')\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # usage: ./download_resources.py CCF_HCD_STG\n",
    "    args = parse_args()\n",
    "\n",
    "    # collections = [\n",
    "    #     #'Diffusion_unproc',\n",
    "    #     #'mbPCASLhr_unproc',\n",
    "    #     # 'rfMRI_REST1_AP_unproc',\n",
    "    #     # 'rfMRI_REST1_PA_unproc',\n",
    "    #     # 'rfMRI_REST2_AP_unproc',\n",
    "    #     # 'rfMRI_REST2_PA_unproc',\n",
    "    #     'Structural_preproc',\n",
    "    #     #       'T1w_MPR_vNav_4e_RMS_unproc',\n",
    "    #     #       'T2w_SPC_vNav_unproc',\n",
    "    #     #       'tfMRI_GUESSING_PA_unproc',\n",
    "    #     #       'tfMRI_GUESSING_AP_unproc',\n",
    "    #     #       'tfMRI_CARIT_AP_unproc',\n",
    "    #     #       'tfMRI_CARIT_PA_unproc',\n",
    "    #     #       'tfMRI_EMOTION_PA_unproc',\n",
    "    # ]\n",
    "\n",
    "    opts = vars(args)\n",
    "    main(**opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "142b70bf-fa04-4703-a41d-2781e3d6a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# Download Resources for a project\n",
    "\n",
    "import argparse\n",
    "from copy import copy\n",
    "from hashlib import md5\n",
    "import logging\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import yaxil\n",
    "from yaxil.exceptions import RestApiError\n",
    "from requests.exceptions import ConnectionError\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "MAX_RETRIES = 3\n",
    "\n",
    "def fetch_experiments(sess, project, subject_labels):\n",
    "    \"\"\"Fetch a list of yaxil.Experiment's (or get all for a project).\"\"\"\n",
    "    logger.info('Fetching list of experiments')\n",
    "    if len(subject_labels):\n",
    "        experiments = []\n",
    "        for label in subject_labels:\n",
    "            try:\n",
    "                sub = list(sess.subjects(label=label, project=project))[0]\n",
    "                experiments.extend(sess.experiments(subject=sub))\n",
    "            except Exception as err:\n",
    "                print('(fetch_experiments) Error with subject {}'.format(label))\n",
    "                print(str(err))\n",
    "    else:\n",
    "        experiments = list(sess.experiments(project=project))\n",
    "    logger.info('Found {} experiments'.format(len(experiments)))\n",
    "    return experiments\n",
    "def fetch_resources(sess, exp_info, collections=None, psychopy=False, scantype=None, subcollection=None, project=None, like_itk=None):\n",
    "    \"\"\"Fetch a list of json resources with collection label and id.\"\"\"\n",
    "    if project == 'CCF_HCD_ITK' or like_itk:\n",
    "        resources_url_pat = ('data/projects/{project}/subjects/{subject_label}/'\n",
    "                         'experiments/{label}/scans')\n",
    "    else:\n",
    "        resources_url_pat = ('data/projects/{project}/subjects/{subject_label}/'\n",
    "                             'experiments/{label}/resources')\n",
    "    base_url = resources_url_pat.format(**exp_info)\n",
    "    \n",
    "    _, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "\n",
    "    # Filter only wanted collections or return all\n",
    "    if collections:\n",
    "        if project == 'CCF_HCD_ITK' or like_itk:\n",
    "            if scantype:\n",
    "                resources = [\n",
    "                    result for result in response['ResultSet']['Result']\n",
    "                    if result['series_description'] in collections and result['type'] in scantype\n",
    "                ]\n",
    "            else:\n",
    "                resources = [\n",
    "                    result for result in response['ResultSet']['Result']\n",
    "                    if result['series_description'] in collections\n",
    "                ]\n",
    "        else:\n",
    "            resources = [\n",
    "                result for result in response['ResultSet']['Result']\n",
    "                if result['label'] in collections\n",
    "            ]\n",
    "    else:\n",
    "        resources = response['ResultSet']['Result']\n",
    "\n",
    "    if not len(resources):\n",
    "        msg = 'Found no resources '\n",
    "        if collections:\n",
    "            msg += 'matching collections {} '.format(collections)\n",
    "        msg += 'for {}'.format(base_url)\n",
    "        logger.warning(msg)\n",
    "    else:\n",
    "        logger.info('Found {} resources matching {}.'.format(str(len(resources)), collections))\n",
    "\n",
    "    return resources\n",
    "def fetch_resource(sess, exp_info, resource_info, always_checksum=False, ignore_list=None, psychopy=False, scantype=None, subcollection=None, project=None, like_itk=None, file_regex=None):\n",
    "    \n",
    "    if not ignore_list:\n",
    "        ignore_list = list()\n",
    "    # Use a cookie to mark a resource as complete\n",
    "    if project == 'CCF_HCD_ITK' or like_itk:\n",
    "        resource_info_dir_index = 'series_description'\n",
    "    else:\n",
    "        resource_info_dir_index = 'label'\n",
    "    success_cookie = os.path.join(exp_info['label'], resource_info[resource_info_dir_index],\n",
    "                                  'SUCCESS')\n",
    "    logger.debug('Fetching resource {}'.format(resource_info[resource_info_dir_index]))\n",
    "    if os.path.exists(success_cookie) and not always_checksum:\n",
    "        return\n",
    "    \n",
    "    if psychopy:\n",
    "        logger.debug('Using intake database URI to fetch resource')\n",
    "        resource_url_pat = (resource_info['URI'] + '/resources/LINKED_DATA/files')\n",
    "        base_url = resource_url_pat\n",
    "    elif project == 'CCF_HCD_ITK' or like_itk:\n",
    "        logger.debug('Using intake database URI to fetch resource')\n",
    "        resource_url_pat = (resource_info['URI'] + '/files')\n",
    "        base_url = resource_url_pat\n",
    "    else:\n",
    "        resource_url_pat = (\n",
    "            'data/projects/{project}/subjects/{subject_label}/'\n",
    "            'experiments/{label}/resources/{xnat_abstractresource_id}/files')\n",
    "        url_info = copy(exp_info)  # Combine resource and experiment\n",
    "        url_info['xnat_abstractresource_id'] = resource_info[\n",
    "            'xnat_abstractresource_id']\n",
    "        base_url = resource_url_pat.format(**url_info)\n",
    "        \n",
    "    _, response = yaxil._get(sess._auth, base_url, yaxil.Format.JSON)\n",
    "\n",
    "    filelist = response['ResultSet']['Result']\n",
    "    if subcollection:\n",
    "        filelist = [\n",
    "            file for file in filelist if file['collection'] in subcollection\n",
    "        ]\n",
    "    if file_regex:\n",
    "        filelist = [\n",
    "            file for file in filelist if re.search(file_regex, file['Name'])\n",
    "        ]\n",
    "        \n",
    "    if not len(filelist):\n",
    "        raise ValueError('No files could be read from {} in json response: {}'.format(base_url, response))\n",
    "    logger.info('Syncing {} file (resources) from {}'.format(len(filelist), base_url))\n",
    "\n",
    "    start_time = time.time()\n",
    "    for fileinfo in filelist:\n",
    "        try:\n",
    "            # Rename URI (python variable case)\n",
    "            fileinfo['uri'] = fileinfo.pop('URI')\n",
    "            if ignore_file(fileinfo['uri'], ignore_list):\n",
    "                logger.debug('Ignoring {}'.format(fileinfo['uri']))\n",
    "                continue\n",
    "            if project == 'CCF_HCD_ITK' or like_itk:\n",
    "                out_dir = os.path.join(exp_info['label'], resource_info[resource_info_dir_index])\n",
    "            else:\n",
    "                out_dir = exp_info['label']\n",
    "            download_file(sess, out_dir=out_dir, **fileinfo)\n",
    "        except RuntimeError:\n",
    "            logger.info('Digest failed on {}'.format(fileinfo['uri']))\n",
    "        except RestApiError as err:\n",
    "            logger.info('Download Error on {}: {}'.format(\n",
    "                fileinfo['uri'], err))\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info('Finished in {}'.format(\n",
    "        time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))))\n",
    "\n",
    "    # if we got here, mark this collection as completed (\"touch\" cookie)\n",
    "    open(success_cookie, 'w').close()\n",
    "def download_file(sess,\n",
    "                  uri,\n",
    "                  digest,\n",
    "                  collection,\n",
    "                  out_dir='.',\n",
    "                  overwrite=False,\n",
    "                  **kwargs):\n",
    "    basename = uri.split('files/')[-1]\n",
    "    fname = os.path.join(out_dir, collection, basename)\n",
    "    dirname = os.path.dirname(fname)\n",
    "\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname)\n",
    "    if os.path.exists(fname):\n",
    "        with open(fname, 'rb') as f:\n",
    "            disk_digest = md5(f.read()).hexdigest()\n",
    "        if disk_digest == digest:\n",
    "            logger.debug('Digest matched - Skipping {}'.format(fname))\n",
    "            return\n",
    "        elif overwrite:\n",
    "            # import pdb; pdb.set_trace()\n",
    "            logger.info(\n",
    "                'Digest failed - removing {} and trying again'.format(fname))\n",
    "            os.remove(fname)\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                '{} exisited with incorrect digest '.format(fname) +\n",
    "                'but cowardly moving on')\n",
    "\n",
    "    try:\n",
    "        _, result = yaxil._get(\n",
    "            sess._auth,\n",
    "            uri,\n",
    "            yaxil.Format.JSON,  # Format is ignored for _file_ downloads\n",
    "            autobox=False)\n",
    "    except RestApiError as err:\n",
    "        # Empty responses are acceptable for some scripts and onset files\n",
    "        if 'response is empty' in str(err):\n",
    "            result = bytes('', 'utf8')\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    with open(fname, 'wb') as f:\n",
    "        logger.info('Writing {}'.format(fname))\n",
    "        f.write(result)\n",
    "\n",
    "    with open(fname, 'rb') as f:\n",
    "        disk_digest = md5(f.read()).hexdigest()\n",
    "    if disk_digest != digest:\n",
    "        retries = kwargs.get('retries', 0)\n",
    "        if overwrite:\n",
    "            os.remove(fname)\n",
    "        if retries >= MAX_RETRIES:\n",
    "            raise RuntimeError(\n",
    "                'Digest failed - ' +\n",
    "                '{} may need to be re-downloaded.'.format(fname))\n",
    "        else:\n",
    "            retries += 1\n",
    "            download_file(sess,\n",
    "                          uri,\n",
    "                          digest,\n",
    "                          collection,\n",
    "                          out_dir,\n",
    "                          overwrite=True,\n",
    "                          retries=retries)\n",
    "\n",
    "\n",
    "def ignore_file(uri, ignores_list):\n",
    "    ignore = False\n",
    "    for ignore_pat in ignores_list:\n",
    "        if re.search(ignore_pat, uri):\n",
    "            ignore = True\n",
    "    return ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a24da029-c891-4b75-ba0b-16759697fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args(args):\n",
    "    parser = argparse.ArgumentParser(description='Download from Remote XNAT')\n",
    "    parser.add_argument('--project', '-p', type=str)\n",
    "    parser.add_argument('--collections', '-c', type=str, nargs='+')\n",
    "    parser.add_argument('--ignore-list', type=str, nargs='+', default=['OTHER_FILES'])\n",
    "    parser.add_argument('--subjects', '-s', type=str, nargs='+', default=[], help='Explicit list of subjects')\n",
    "    parser.add_argument('--sessions', type=list, default=[], help='Explicit list of sessions')\n",
    "    parser.add_argument('--no-checksum', action='store_true')\n",
    "    parser.add_argument('--scantype', '-t', type=str, nargs='+', help='Specify scan type; useful for intake project.')\n",
    "    parser.add_argument('--subcollection', '-C', type=str, nargs='+', help='Specify sub-collection; useful for intake project.')\n",
    "    parser.add_argument('--psychopy', action='store_true', help='Download behavior data from intake project for specified collections')\n",
    "    parser.add_argument('--like-itk', action='store_true', help='Download using intake-style urls (useful for getting special scan data even from staging project)')\n",
    "    parser.add_argument('--file-regex', '-r', type=str, help='Grab files by regex. This regex is not checked!')\n",
    "    return parser.parse_args(args)\n",
    "\n",
    "parsed_args = parse_args(['-p', 'CCF_HCD_STG', '-c', 'tfMRI_CARIT_PA', 'tfMRI_CARIT_AP', '--psychopy', '-s', 'HCD0015417'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17979c91-1744-4a55-aa35-f92545dc299b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(collections=['tfMRI_CARIT_PA', 'tfMRI_CARIT_AP'], file_regex=None, ignore_list=['OTHER_FILES'], like_itk=False, no_checksum=False, project='CCF_HCD_STG', psychopy=True, scantype=None, sessions=[], subcollection=None, subjects=['HCD0015417'])\n"
     ]
    }
   ],
   "source": [
    "print(parsed_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd087312-3c5d-4ca1-9766-f023e8263e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = vars(parsed_args)\n",
    "locals().update(opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef840e9f-99a9-46dc-afcb-0637e0787c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Project is CCF_HCD_STG.\n",
      "INFO:__main__:Downloading psychopy files from intake project\n"
     ]
    }
   ],
   "source": [
    "if not ignore_list:\n",
    "    ignore_list = list()\n",
    "if not subjects:\n",
    "    subjects = list()\n",
    "if not sessions:\n",
    "    sessions = list()\n",
    "if not scantype:\n",
    "    scantype = list()\n",
    "if not subcollection:\n",
    "    subcollection = list()\n",
    "if no_checksum:\n",
    "    always_checksum=False\n",
    "else:\n",
    "    always_checksum=True\n",
    "logger.info('Project is {}.'.format(project))\n",
    "if psychopy:\n",
    "   logger.info('Downloading psychopy files from intake project')\n",
    "auth = yaxil.auth('intradb')  # Requires setup and description\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bf134e1-da51-4be8-8721-a8a064dc7eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<contextlib._GeneratorContextManager object at 0x152c900dab90>\n",
      "CCF_HCD_STG\n",
      "['HCD0015417']\n"
     ]
    }
   ],
   "source": [
    "sess = yaxil.session(auth)\n",
    "print(sess)\n",
    "print(project)\n",
    "print(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d09da50-cb45-4d00-ad5f-40ed0137fc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Fetching list of experiments\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/subjects\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/subjects?columns=ID%2Clabel%2Cproject&label=HCD0015417&project=CCF_HCD_STG HTTP/1.1\" 200 None\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments?columns=ID%2Clabel%2Cproject%2Cxnat%3Asubjectassessordata%2Fsubject_id%2Csubject_label%2Cinsert_date&project=CCF_HCD_STG&xnat%3Asubjectassessordata%2Fsubject_id=HCPIntradb04_S00550 HTTP/1.1\" 200 None\n",
      "INFO:__main__:Found 3 experiments\n"
     ]
    }
   ],
   "source": [
    "with yaxil.session(auth) as sess:\n",
    "    experiments = fetch_experiments(sess, project, subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b80c1522-653f-4a7c-a797-3e7a5fd6c488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Experiment(uri='/data/experiments/HCPIntradb04_E05816', label='HCD0015417_V1_MR', id='HCPIntradb04_E05816', project='CCF_HCD_STG', subject_id='HCPIntradb04_S00550', subject_label='HCD0015417', archived_date='2018-12-03 10:51:40.293'),\n",
       " Experiment(uri='/data/experiments/HCPIntradb09_E00969', label='HCD0015417_V2_MR', id='HCPIntradb09_E00969', project='CCF_HCD_STG', subject_id='HCPIntradb04_S00550', subject_label='HCD0015417', archived_date='2020-07-02 08:50:09.195'),\n",
       " Experiment(uri='/data/experiments/HCPIntradb54_E00007', label='HCD0015417_V3_MR', id='HCPIntradb54_E00007', project='CCF_HCD_STG', subject_id='HCPIntradb04_S00550', subject_label='HCD0015417', archived_date='2021-05-24 10:10:51.989')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ccdf08f-4167-4262-bb95-e76387e0fe33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_infos = [e._asdict() for e in experiments]\n",
    "exp_info = exp_infos[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "299c06d5-dc24-4d12-af89-0340ea7b661b",
   "metadata": {},
   "outputs": [],
   "source": [
    "like_itk = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46f952a8-18fe-4e31-8a62-767f7d04cfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Syncing experiment HCD0015417_V1_MR\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/projects/CCF_HCD_STG/subjects/HCD0015417/experiments/HCD0015417_V1_MR/scans\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/projects/CCF_HCD_STG/subjects/HCD0015417/experiments/HCD0015417_V1_MR/scans?format=json HTTP/1.1\" 200 None\n",
      "INFO:__main__:Found 2 resources matching ['tfMRI_CARIT_PA', 'tfMRI_CARIT_AP'].\n"
     ]
    }
   ],
   "source": [
    "logger.info('Syncing experiment {}'.format(exp_info['label']))\n",
    "start_time = time.time()\n",
    "with yaxil.session(auth) as sess:\n",
    "    resource_errors = []\n",
    "    try:\n",
    "        resources = fetch_resources(sess, exp_info, collections, psychopy=psychopy, scantype=scantype, subcollection=subcollection, project=project, like_itk=like_itk)\n",
    "    except ValueError:\n",
    "        logger.error('Unrecoverable error in {}'.format(exp_info['label']))\n",
    "        resources = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2f0e12f-3f2c-4d03-a176-9a8302c28538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'xsiType': 'xnat:mrScanData',\n",
       "  'xnat_imagescandata_id': '621723',\n",
       "  'note': '',\n",
       "  'series_description': 'tfMRI_CARIT_PA',\n",
       "  'ID': '122',\n",
       "  'type': 'tfMRI',\n",
       "  'URI': '/data/experiments/HCPIntradb04_E05816/scans/122',\n",
       "  'quality': 'usable'},\n",
       " {'xsiType': 'xnat:mrScanData',\n",
       "  'xnat_imagescandata_id': '621725',\n",
       "  'note': '',\n",
       "  'series_description': 'tfMRI_CARIT_AP',\n",
       "  'ID': '124',\n",
       "  'type': 'tfMRI',\n",
       "  'URI': '/data/experiments/HCPIntradb04_E05816/scans/124',\n",
       "  'quality': 'usable'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "504418eb-c40c-43e4-9bf1-56fe8f4436be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xsiType': 'xnat:mrScanData',\n",
       " 'xnat_imagescandata_id': '621723',\n",
       " 'note': '',\n",
       " 'series_description': 'tfMRI_CARIT_PA',\n",
       " 'ID': '122',\n",
       " 'type': 'tfMRI',\n",
       " 'URI': '/data/experiments/HCPIntradb04_E05816/scans/122',\n",
       " 'quality': 'usable'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resource = resources[0]\n",
    "resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25c71065-9258-409f-b115-ac0b31197d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:__main__:Fetching resource tfMRI_CARIT_PA\n",
      "DEBUG:__main__:Using intake database URI to fetch resource\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/LINKED_DATA/files\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/LINKED_DATA/files?format=json HTTP/1.1\" 200 None\n",
      "INFO:__main__:Syncing 10 file (resources) from /data/experiments/HCPIntradb04_E05816/scans/122/resources/LINKED_DATA/files\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_design.csv\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_design.csv?format=json HTTP/1.1\" 200 2376\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_design.csv\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PHYSIO/Physio_combined_48940556-dacb-4c8c-b0d5-95bf9388134d.csv\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PHYSIO/Physio_combined_48940556-dacb-4c8c-b0d5-95bf9388134d.csv?format=json HTTP/1.1\" 200 1555826\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PHYSIO/Physio_combined_48940556-dacb-4c8c-b0d5-95bf9388134d.csv\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_wide.csv\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_wide.csv?format=json HTTP/1.1\" 200 28958\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_wide.csv\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoFALose.txt\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoFALose.txt?format=json HTTP/1.1\" 200 14\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/EVs/nogoFALose.txt\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/go.txt\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/go.txt?format=json HTTP/1.1\" 200 941\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/EVs/go.txt\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/miss.txt\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/miss.txt?format=json HTTP/1.1\" 200 0\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/EVs/miss.txt\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoCRLose.txt\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoCRLose.txt?format=json HTTP/1.1\" 200 154\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/EVs/nogoCRLose.txt\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoFAWin.txt\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoFAWin.txt?format=json HTTP/1.1\" 200 41\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/EVs/nogoFAWin.txt\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoCRWin.txt\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/EVs/nogoCRWin.txt?format=json HTTP/1.1\" 200 125\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/EVs/nogoCRWin.txt\n",
      "DEBUG:yaxil:issuing http request https://intradb.humanconnectome.org/data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_stats.csv\n",
      "DEBUG:yaxil:query parameters {'format': 'json'}\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): intradb.humanconnectome.org:443\n",
      "DEBUG:urllib3.connectionpool:https://intradb.humanconnectome.org:443 \"GET /data/experiments/HCPIntradb04_E05816/scans/122/resources/2098130/files/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_stats.csv?format=json HTTP/1.1\" 200 512\n",
      "INFO:__main__:Writing HCD0015417_V1_MR/tfMRI_CARIT_PA/LINKED_DATA/PSYCHOPY/CARIT_HCD0015417_V1_A_run1_stats.csv\n",
      "INFO:__main__:Finished in 00:00:06\n"
     ]
    }
   ],
   "source": [
    "fetch_resource(sess, exp_info, resource, always_checksum=always_checksum, ignore_list=ignore_list, psychopy=psychopy, scantype=scantype, subcollection=subcollection, project=project, like_itk=like_itk, file_regex=file_regex)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7d7e6-8794-4c26-a44d-920c1790f240",
   "metadata": {},
   "outputs": [],
   "source": [
    "resource_errors = []\n",
    "try:\n",
    "    resources = fetch_resources(sess, exp_info, collections, psychopy=psychopy, scantype=scantype, subcollection=subcollection, project=project, like_itk=like_itk)\n",
    "except ValueError:\n",
    "    logger.error('Unrecoverable error in {}'.format(exp_info['label']))\n",
    "    resources = []\n",
    "for resource in resources:\n",
    "    logger.debug('Fetching all the resources...')\n",
    "    try:\n",
    "        fetch_resource(sess, exp_info, resource, always_checksum=always_checksum, ignore_list=ignore_list, psychopy=psychopy, scantype=scantype, subcollection=subcollection, project=project, like_itk=like_itk, file_regex=file_regex)\n",
    "    except ValueError as err:\n",
    "        if 'No JSON object could be decoded' in str(err):\n",
    "            logger.error(err)\n",
    "            continue\n",
    "        else:\n",
    "            raise\n",
    "    except ConnectionError as err:\n",
    "        logger.error(err)\n",
    "        resource_errors.append('ConnectionError: {}'.format(\n",
    "            exp_info['label']))\n",
    "        continue\n",
    "\n",
    "with open('errors.log', 'a') as f:\n",
    "    f.writelines(resource_errors)\n",
    "elapsed_time = time.time() - start_time\n",
    "logger.info('Finished experiment {} in {}'.format(\n",
    "    exp_info['label'], time.strftime(\"%H:%M:%S\",\n",
    "                                     time.gmtime(elapsed_time))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hcpl",
   "language": "python",
   "name": "hcpl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
